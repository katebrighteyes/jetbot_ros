1. ROS melodic

sudo sh -c 'echo "deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main" > /etc/apt/sources.list.d/ros-latest.list'

sudo apt-key adv --keyserver 'hkp://keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654

curl -sSL 'http://keyserver.ubuntu.com/pks/lookup?op=get&search=0xC1CF6E31E6BADE8868B172B4F42ED6FBAB17C654' | sudo apt-key add -

sudo apt-get update

sudo apt-get install ros-melodic-ros-base

sudo sh -c 'echo "source /opt/ros/melodic/setup.bash" >> ~/.bashrc'

sudo apt-get install python-pip

$ sudo apt-get install python-pip

# install Adafruit libraries
$ pip install Adafruit-MotorHAT
$ pip install Adafruit-SSD1306
Grant your user access to the i2c bus:

$ sudo usermod -aG i2c $USER


sudo sh -c 'echo "source ~/catkin_ws/devel/setup.bash" >> ~/.bashrc'

-------------------------
2 jetson-inference

sudo apt-get install git cmake

git clone https://github.com/dusty-nv/jetson-inference

cd jetson-inference
git submodule update --init

# build from source
mkdir build
cd build
cmake ../
make

# install libraries
sudo make install
sudo ldconfig

----------------------------
3. jetbot ros_deep_learning

sudo apt-get install ros-melodic-vision-msgs ros-melodic-image-transport ros-melodic-image-publisher

# clone the repo
cd ~/catkin_ws/src
git clone https://github.com/dusty-nv/ros_deep_learning

# make ros_deep_learning
cd ../    # cd ~/workspace/catkin_ws
catkin_make

# confirm that the package can be found
$ rospack find ros_deep_learning
/home/jetbot/catkin_ws/src/ros_deep_learning

-----------------------------------
4. jetbot ros
$ cd ~/catkin_ws/src
$ git clone https://github.com/dusty-nv/jetbot_ros

# build the package
$ cd ../    # cd ~/workspace/catkin_ws
$ catkin_make

# confirm that jetbot_ros package can be found
$ rospack find jetbot_ros
/home/jetbot/workspace/catkin_ws/src/jetbot_ros

---------------------------------

5. Testing JetBot
Next, let's check that the different components of the robot are working under ROS.

First open a new terminal, and start roscore

$ roscore
Running the Motors
Open a new terminal, and start the jetbot_motors node:

$ rosrun jetbot_ros jetbot_motors.py
The jetbot_motors node will listen on the following topics:

/jetbot_motors/cmd_dir relative heading (degree [-180.0, 180.0], speed [-1.0, 1.0])
/jetbot_motors/cmd_raw raw L/R motor commands (speed [-1.0, 1.0], speed [-1.0, 1.0])
/jetbot_motors/cmd_str simple string commands (left/right/forward/backward/stop)
Note: currently only cmd_str method is implemented.

Test Motor Commands
Open a new terminal, and run some test commands:

$ rostopic pub /jetbot_motors/cmd_str std_msgs/String --once "forward"
$ rostopic pub /jetbot_motors/cmd_str std_msgs/String --once "backward"
$ rostopic pub /jetbot_motors/cmd_str std_msgs/String --once "left"
$ rostopic pub /jetbot_motors/cmd_str std_msgs/String --once "right"
$ rostopic pub /jetbot_motors/cmd_str std_msgs/String --once "stop"

-------------------------------------------

6.  OLED
If you have an SSD1306 debug OLED on your JetBot, you can run the jetbot_oled node to display system information and user-defined text:

$ rosrun jetbot_ros jetbot_oled.py
By default, jetbot_oled will refresh the display every second with the latest memory usage, disk space, and IP addresses.

The node will also listen on the /jetbot_oled/user_text topic to recieve string messages from the user that it will display:

rostopic pub /jetbot_oled/user_text std_msgs/String --once "HELLO!"

------------------
7. camera

$ rosrun jetbot_ros jetbot_camera
The video frames will be published to the /jetbot_camera/raw topic as sensor_msgs::Image messages with BGR8 encoding. To test the camera feed, install the image_view package and then subscribe to /jetbot_camera/raw from a new terminal:

# first open a new terminal
$ sudo apt-get install ros-melodic-image-view
$ rosrun image_view image_view image:=/jetbot_camera/raw
